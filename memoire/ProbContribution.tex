% !TEX root = ./memoire/main.tex
\chapter{Méthodes particulaires et assimilation de données}

L'assimilation de données, lorsqu'appliquée à des systèmes simulés par la méthode des éléments discrets (DEM), se heurte à plusieurs limites importantes. A la fois pour les méthodes variationnelles (ex : 3DVar) ou stochastique (ex : EnKF) il faut considérer deux problématiques majeurs : la définition de l'état et la manière de le mettre à jour.

\section{Problématique et revue bibliographique}

\subsection{Définition de l'état d'une méthode particulaire}~\label{sec:etat_meshless}

L'état est ce qui permet de représenter l'état du système virtuel.

Les méthodes classiques vont utiliser un maillage fixe pour le définir.

En considérant un domaine $\Omega \subset \mathbb R^d$, soit $\mathcal{T} = (\mathcal N, \mathcal E)$ un maillage, où $\mathcal N = \left\{\bm x_i \right\}^N_{i=1}$ est l'ensemble des nœuds, et $\mathcal{E} = \{ K_j \}_{j=1}^{M}$ l'ensemble des éléments.

L'état de la solution \(u_h\) discrétisé peut alors être définie

\begin{itemize}
    \item sur les nœuds du maillage $\bz = \{ u(\mathbf{x}_i) \}_{i=1}^{N}$, c'est le cas des éléments finis (FEM) ;
    \item  sur les éléments du maillage $\bz = \{ u(K_j) \}_{j=1}^{M}$, c'est le cas des volumes finis (VF).
\end{itemize}

Le maillage étant fixe, il n'est pas nécessaire d'inclure la position des nœuds ou des éléments.

Il est alors possible de définir l'état comme une variable aléatoire et d'exprimer ses statistiques.

Dans le cas des méthodes particulaires, la situation est différentes. Le support de discrétisation, porté par les particules, évolue au cours du temps. L'état ne peut donc pas être défini uniquement par les valeurs d'intensité, mais doit considérer également les positions.

Une première solution consiste à considérer chaque particules indépendamment dans la définition de l'état.

Pour la DEM, l'état à l’instant $t$, pourrait alors être défini comme

\begin{equation*}
    \bz = {(\bx_i(t), \bm v_i(t), \bm \theta_i, \bm \omega, \mathcal P_i)}_{i=1}^N
\end{equation*}où $N$ le nombre de particules, et présente à la fois les variables cinématiques ainsi que $\mathcal P_i$ les propriétés intrinsèques de la particules comme sa masse ou sa géométrie.

De même pour les méthodes particulaires continues, par exemple la méthode vortex, l'état peut être défini comme

\begin{equation*}
    \bz = {(\bx_i(t),\bm \Gamma_i(t), \mathcal P_i)}_{i=1}^N
\end{equation*}où cette fois $\mathcal P_i$ défini en particulier les propriétés de la fonction de lissage.

Cette définition de l'état a en particulier été utilisé~\cite{chen_superfloe_2022} pour une méthode DEM appliqué au mouvement de la banquise ou bien avec la méthode vortex~\cite{darakananda_data-assimilated_2018,le_provost_ensemble_2021}. C'est sont des filtres EnKF qui ont été présenté à chaque fois.

Toutefois, cette définition peut difficilement être mise en pratique dans le cas général en particulier pour les méthodes d'ensemble.

Premièrement, il est alors nécessaire de faire l'hypothèse que tous les membres possèdent le même nombre de particules dans un ordre défini.
Dans le cas contraire il n'est pas possible de réaliser le calcul des statistiques sur l'ensemble.

Dans le cas où les membres disposerait d'un nombre de particules différents, il serait possible d'ajouter les particules supplémentaires à la suite des précédentes dans le vecteur d'état en remplissant de valeurs nulles pour les autres membres. Cependant, le risque est de voir le nombre de particules augmenter considérablement.

Cette remarque est d'autant plus sensible en considérant le caractère chaotique d'un problème à N corps~\cite{poincare1890}. En effet, dans ce cas, deux particules proches à une perturbation près peuvent, en un temps, voir leur trajectoires totalement décorrélée~\textcolor{red}{image d'un système à N corps}. Le système peut conserver une régularité dans sa globalité, mais il devient tout à fait incongru de continuer à comparer ces deux particules entre elles. Ainsi, il n'est plus possible de définir l'état comme selon une collection de particules individuelles ou bien de définir des statistiques qui est un sens pour une particule données.

Dans les articles~\cite{chen_superfloe_2022} ou~\cite{darakananda_data-assimilated_2018} cette première limitation a été traité en regroupant les particules pour en obtenir des systèmes de plus faible dimension. Dans le premier cas, il s'agit de créer des superstructures de glace, dans le second cas, il s'agit d'utiliser des méthodes d'agglomération de vortex.

Néanmoins, ces méthodes réduisent le niveau de détail de la discrétisation. Dans notre cas, nous souhaiterions proposer des méthodes d'assimilation qui ne nécessite pas de réduire la qualité des solutions.

\subsection{Mise à jour de l'état d'une simulation particulaire}

\subsubsection{Pour les méthodes variationnelles}

Une autre problématique majeur est celle de la mise à jour de l'état lors de l'analyse.
Une première limitation concerne les méthodes variationnelle comme la méthodes 3DVar. Pour ce type de problème, directement l'état à partir des variables particulaires est problématique pour différentes raison. Tout d'abord, l'opérateur d'observation $\mathcal H(z)$, s'il consiste à observer un champ par exemple, est non-linéaire par rapport aux variables particulaires tel que la position $\bx_i$ ou l'orientation $\bm \theta_i$. Pour un système avec des milliers voire des millions de particules, on se retrouve à traiter un problème d'optimisation fortement non-linéaire dans un espace de très grande dimension.

De nouveau, on pourrait procéder à une réduction du nombre de particules.

Toutefois, dans le cas des méthodes discrètes, il est nécessaire de prendre en considération un nombre élevé de contraintes. Notamment, le déplacement des particules doit respecter l'interdiction d'interpénétration des particules. Cette conditions doit être vérifié sur l'ensemble du système pour assurer que la solution d'optimisation soit physiquement admissible. Dans le cas des méthodes continues, les particules sont des entités ponctuelles et ne représente qu'une discrétisation d'un milieu continu. Ainsi cette dernière contrainte ne s'applique.

Finalement, utiliser une méthode d'assimilation de données variationnelles dans le cas d'une discrétisation particulaire nécessite de traiter une problème d'optimisation non linéaire de grande dimension avec contraintes lorsque le milieu est discret et sans contrainte dans le cas continu.

\subsubsection{Pour la méthode EnKF}

Dans le cas de la méthode EnKF, la mise à jour est définie de manière statistique. L'état estimé du système est une combinaison linéaire des états prédits par les différents membres de l'ensemble, en particulier sur les positions des particules $\bx_i$. Or bien que chaque membre vérifie les conditions de d'interpénétration, ses conditions n'étant pas linéaire, il n'y a aucune garantie pour que la combinaison le soi.
En d'autres terme, la mise à jour ne peut être réalisé que via le solveur lui-même capable de vérifier des contraintes de non interpénétration mais ne peut être réalisé directement. Dans le cas de l'article~\cite{chen_superfloe_2022}, nous supposons que le critère de pénétration est relaxé que pour des simulations de milieux granulaire.

Si toutefois cette problématique ne concerne pas les méthodes particulaires continues, il reste toutefois questionnable de réaliser une combinaison linéaire des positions des particules. Cette question est en fait en lien avec la définition de l'état.
En effet, chaque particule n'est que le support de la discrétisation d'un champ. Mesurer une erreur quadratique sur la position des particules n'aura pas de véritable avec l'erreur quadratique sur le champ.
Ceci est relativement vrai pour des champs tournant. La figure~\textcolor{red}{rajouter une exemple}, présente un cas où chaque membre discrétise un même champ de tourbillon mais où le support de discrétisation a plus ou moins de retard. L'estimateur sur une grille eulérienne sera bien ce même champ. Or, dans le cas ou ce sont les quantités de la discrétisation qui sont prise en compte dans l'état et l'estimateur ne sera par représentatif.
Encore une fois, une solution consiste à réaliser des agrégations pour regrouper les particules comme en~\cite{chen_superfloe_2022,darakananda_data-assimilated_2018} mais au détriment du détail de discrétisation.

Finalement, la mise à jour en considérant des discrétisations particulaires est complexe. En DEM elle doit prendre en considération les positions relatives des particules entre elles. De manière générale, nous avons vu que considérer les statistiques sur les positions de particules ne permettait pas d'avoir une bonne idée de l'incertitude sur le champ discrétisé.

Ainsi, il nous semble nécessaire de formuler différemment le problème d'assimilation dans le cadre d'un discrétisation particulaire.

\subsection{Assimilation de données sur maillages adaptatifs}

\textcolor{red}{présenter des méthodes dans des cas où les membres ne partage pas la même discrétisation}

Les méthodes particulaires continues peuvent être considérées comme le choix d'une discrétisation mobile d'un problème continu. Ainsi, en utilisant un ensemble de simulations basées sur ces méthodes, on se heurte aux mêmes problématiques que celles rencontrées avec des simulations utilisant des maillages adaptatifs.
Dans ce cas le principal défi réside dans le fait que les dimensions de l'espace d'état changent au cours du temps et diffèrent d'un membre de l'ensemble à l'autre. Cette variabilité complique les opérations ensemblistes habituelles, telles que les calculs matriciels, lors de la mise à jour de l'analyse.

Une solution courante consiste à adopter une discrétisation de référence commune à tous les membres. Dans les méthodologies à grille fixe avec l'analyse multi-résolution (MRA) et dans les scénarios de maillage mobile, les états définis sur des grilles variées sont harmonisés à travers des techniques de projection et d'interpolation pour établir une grille de référence destinée aux mises à jour des états \cite{siripatana_combining_2019, bonan_data_2017}. Cette démarche offre une grande flexibilité, permettant diverses combinaisons de grilles de référence et de mise à jour adaptées aux spécificités des données et des objectifs de l'étude. Par ailleurs, Siripatana et al. \cite{siripatana_combining_2019} soulignent que la correction par le Filtre de Kalman d’Ensemble (EnKF) repose uniquement sur les prédictions et les observations, indépendamment de la définition de l'état. En réalité, cela simplifie l'application de l'EnKF dans des contextes où les maillages varient entre les membres de l'ensemble en proposant une correction qui est indépendante de la discrétisation

\subsection{Méthodes d'assimilation pour des données lagrangiennes}
\textcolor{red}{présenter des méthodes dans des cas où les variables sont des quantités lagrangiennes}

D'autre part
- Méthodes particulaires sont des méthodes lagrangiennes
- Avoir données variable problématique déjà traité par des données océanographiques issu de flotteurs par exemple
- Pour traiter ce type de problème, une première approche initialement développé dans~\cite{ide_2002}, est d'augmenter le vecteur d'état ici noté $\bm x$ avec les données d'observation de position de traceur. Cependant, cette variable étant fortement non-linéaire, l'application d'un filtre de Kalman étendu (linéarisation du filtre de Kalman) ne converge pas si les observations ne sont pas suffisamment fréquente. Finalement, cela revient à définir l'état comme dans la Section~\ref{sec:etat_meshless} précédente, ce qui n'est pas adéquat.

L'utilisation d'une méthode de type état augmenté trouve plus de sens pour l'application de méthode variationnel.
C'est en particulier ce qui est proposé dans les travaux de Apte~\cite{apte_2008} pour le filtre 3DVar et le ceux de Nodet~\cite{nodet_2006} pour le filtre 4DVar incrémental.

Cependant, ici les quantités lagrangiennes sont des quantités passives, c'est à dire qu'elles n'interviennent pas dans la résolution du champ de vitesse à intégrer. Dans notre cas les traceurs sont des supports de discrétisation de la solution. De ce fait, le modèle adjoint du modèle n'est pas aussi simple à construire car la position des particules impactent le champ de vitesse à intégrer.


\subsection{Méthodes d'assimilation de données par déplacement ou alignement}~\label{sec:biblio_align}

Dans le Chapitre~\ref{sec:da}, nous avons présenté l'assimilation de données comme la combinaison les simulations issues d'un modèle avec les données bruités issues de l'observation. Cette combinaison nécessite de pouvoir mettre à jour de manière optimale l'état de la simulation en tenant compte à la fois de l'erreur \textit{a priori} du modèle ainsi que l'erreur d'observation. Plus précisément, ces méthodes cherchent à déterminer la distribution ou l'estimateur du maximum a posteriori (MAP) de l'état par correction de l'intensité.

Cependant, on trouve de nombreuses limitations à ces schémas classiques, en particulier lorsqu'il existe une erreur de position. En effet, les méthodes d'assimilation de données classiques sont construites en mesurant l'erreur à l'aide d'une norme quadratique qui tend à sur pénaliser des erreurs issues d'alignement. Pour s'en rendre compte, on peut constater en Figure~\ref{fig:double_penalization_error} qu'une erreur sur l'alignement sera sur-pénaliser en comparaison de l'erreur avec une solution nulle. On parle d'effet de \textit{double pénalisation} car cette effet intervient à la fois pour l'évaluation de l'erreur sur le modèle mais également sur les observations~\cite{amodei2009}. C'est d’ailleurs une des contribution majeur à l'erreur de représentativité~\cite{janjic2018}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{double_penalisation.png}
    \caption{Visualisation de l'effet de double pénalisation.}~\label{fig:double_penalization_error}
\end{figure}

Un seconde limitation, concerne la nécessité pour les champs d'état et d'observation \textit{a priori} de recouvrir la solution à analyser. En effet, la mise à jour des méthodes d'assimilation de données classique consiste essentiellement à un interpolation dans l'espace des valeurs des champs, produisant ainsi une analyse encore confinée dans le support de l'état de fond et celui de l'observation. Cette remarque est d'autant plus flagrante dans le cas du filtre EnKF qui se défini justement comme combinaison des membres de l'ensemble comme décrit en Section~\ref{sec:membre}. Cette limitation est donc tout à fait en lien avec notre problème de support de discrétisation particulaire.


Pour résoudre ce problème plusieurs approches ont pu être proposées. Une première manière assez élégante est de s'inspirer de méthode de transport optimal pour définir une correction dans un espace d'interpolation plus riche et prenant en compte des déplacements~\cite{villani2009optimal,benamou_computational_2000}. Ceci passe par substitution de la norme quadratique par une norme de Wasserstein. Ainsi les travaux de thèse de Feyeux~\cite{feyeux_transport_2016} ont permis d'utiliser une distance de Wasserstein pour l'assimilation de données issues d'images. L'article de Bocquet et al. \cite{bocquet_bridging_2023} propose quand à lui une adaptation de la méthode 3D-Var au transport optimal.En particulier, son approche s'applique à des distributions d'état et d'observation de masses potentiellement différentes. Bien que les algorithmes de transport optimal ait gagnés en efficacité computationnel~\cite{cuturi_2014,peyre_cuturi_2019,Simsekli2018SlicedWassersteinFN}, ils restent encore difficilement applicable dans le contexte de l'assimilation de données.

Si le transport optimal permet en effet de réaliser simultanément des corrections en intensité et en position, on trouve un certain nombre de développement qui introduisent des transformations spatiales. En particulier Percival et al.~\cite{percival_department_2008} utilisent des idées en théorie du réarrangement afin d'appliquer une transformation de coordonnées. Ravela et al.~\cite{ravela_data_2007} introduisent de leur côté une variable de déplacement pour ainsi corriger indépendamment position et intensité. Cette méthode à aussi été adaptée dans une formulation multi-échelle~\cite{ying_multiscale_2019,ying_improving_2023}. Finalement, Rosenthal et al.~\cite{rosenthal_displacement_2017} présente une méthode séquentielle en deux étapes pour aligner et corriger les intensité successivement. L'objectif est alors de conserver des propriétés morphologiques de tourbillon. Pour cela, la correction est réalisée en appliquant une transformation cinématiquement admissible pour corriger la position, puis une correction d'intensité. L'avantage de cette méthode est qu'elle offre la possibilité de la coupler aux méthodes classiques d'assimilation.

C'est donc dans la continuité de ces travaux, et en particulier ceux de Rosenthal, que nous souhaitons proposer une méthode de correction de distribution particulaire des simulations sans maillage.

\section{Bilan du chapitre}

Au travers des précédentes méthodes particulaires, nous constatons que l'application des méthodes d'assimilation sont inégalement applicable. En particulier, les méthodes discrètes offre difficilement la possibilité de corriger directement l'état de la discrétisation, et nécessite une correction au travers du schémas d'intégration pour éviter des problèmes d'interpénétration. Dans la suite, nous nous focaliserons sur les méthodes particulaires continues.

D'autre part, les méthodes particulaires continues (par exemple MPM, SPH), permettent une plus grande flexibilité des schémas d'assimilation. En effet, chaque particule est définie en un point, ce qui annule tout problème d'interpénétration.

Dans tous les cas, ce sont des méthodes qui ont la particularité d'avoir des discrétisations variables au cours du temps. Cela vient de la représentation lagrangienne sous-jacente. Cela rend difficile l'application de méthodes ensemblistes comme la méthode EnKF ou des opérations de calcul matriciel doivent être réalisé entre membres.

D'autre part, ce sont des méthodes dont les opérateurs d'observation sont hautement non-linéaire par rapport aux positions des particules. Ainsi, cela complexifie l'application de méthodes variationnelles qui doivent construire des modèles adjoints pour des méthodes particulaires de grande taille.

Ainsi il devient assez claire que positions et intensités de particule ne peuvent pas être traité de la même manière. Ainsi en s'inspirant des méthodes d'assimilation sur des maillages adaptatifs ou multi résolution, on proposera des méthodes d'assimilation ensembliste qui mettrons à jour les intensités.

Dans un second temps, on proposera plutôt des approches variationnelles pour traiter de la correction des positions de particule.

Afin de facilité le développement de nouveaux filtres, nous avons utilisé la méthode vortex. Elle a été choisie car elle offre une modélisation plus simple que les méthodes SPH et MPM. En effet, chaque particule ne transporte qu'une quantité scalaire, une quantité de tourbillon. Toutefois, elle dispose de toutes les caractéristiques d'une méthode particulaire continue avec à la fois une formulation classique de la méthode se rapproche de la méthode SPH et la méthode Vortex-In-Cell de la méthode MPM.
